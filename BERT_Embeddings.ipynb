{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pd.read_csv(\"fpc_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids_masks(df, tokenizer, model):\n",
    "    # tokenize the feature column 'word'\n",
    "    tokenized = df['word'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "\n",
    "    # pad input text to same length (with the token id 0)\n",
    "    padded = pad_sequences(tokenized, padding='post')\n",
    "    \n",
    "    # create attention masks\n",
    "    # attend to useful, real tokens only (represented by 1)\n",
    "    mask = [[int(token_id > 0) for token_id in sentence] for sentence in padded]\n",
    "    mask = np.asarray(mask)\n",
    "\n",
    "    # convert data to torch tensors\n",
    "    input_ids = torch.tensor(padded)\n",
    "    input_mask = torch.tensor(mask)\n",
    "    \n",
    "    return input_ids, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_masks = get_ids_masks(data, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids.long(), attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer number, max token number, number of hidden unit / feature \n",
    "print(last_hidden_states[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv file\n",
    "features_df = pd.DataFrame(features)\n",
    "features_df.to_csv(\"feature_embeddings.csv\", encoding='utf-8', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* To concatenate all the fpc files into one dataframe, check out this [thread](https://stackoverflow.com/questions/20906474/import-multiple-csv-files-into-pandas-and-concatenate-into-one-dataframe) on Stack Overflow.\n",
    "* An alternative to using the ```get_ids_masks``` function to get the input_ids and attention_masks is to use ```tokenizer.encode_plus```. The function has the ability to do everything listed in the ```get_ids_masks``` function. Check out the [documentation](https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
