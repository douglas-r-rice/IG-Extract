{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "* Follow the steps in the <a href=\"https://gitlab.com/c2lab_/freygroup/learningtogovern\" target=\"_blank\">learningtogovern repo</a> to obtain the typology data.\n",
    "\n",
    "* Please run the Python script on GPU as performance on CPU will be slow. You can get free access to GPUs through Google Colab.\n",
    "\n",
    "* We will be using the pretrained BERT model `BertForSequenceClassification` to detect institutional statements. The expected validation accuracy for IS detection is 0.91667 and the expected train loss is 0.2. \n",
    "\n",
    "Reference:\n",
    "* https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
    "\n",
    "* https://skimai.com/fine-tuning-bert-for-sentiment-analysis/\n",
    "\n",
    "* https://gitlab.com/c2lab_/freygroup/rule-norm-strategy-classifier/-/tree/master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim import utils\n",
    "from sklearn.svm import LinearSVC\n",
    "from markdown import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn import preprocessing\n",
    "import gensim.parsing.preprocessing as gsp\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import cross_validate, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "import torch\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_TYPOLOGY_PATH = \"training_typology.csv\"\n",
    "TESTING_TYPOLOGY_PATH = \"testing_typology.csv\"\n",
    "\n",
    "training_df = pd.read_csv(TRAINING_TYPOLOGY_PATH)\n",
    "testing_df = pd.read_csv(TESTING_TYPOLOGY_PATH)\n",
    "\n",
    "full_df = pd.concat([training_df,testing_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitespace_removal(df):\n",
    "    \"\"\"\n",
    "    remove white space from columns 'rule_norm_strategy' and 'reg_const'\n",
    "    \"\"\"\n",
    "\n",
    "    df.rule_norm_strategy = df.rule_norm_strategy.apply(lambda x: x.strip())\n",
    "    df.reg_const = df.reg_const.apply(lambda x: x.strip())\n",
    "    return df\n",
    "\n",
    "def strip_html_markdown(s):\n",
    "    if type(s) not in [int, float] and s is not None:\n",
    "        return (\n",
    "            \" \".join(\n",
    "                re.split(\n",
    "                    \"[ _<>,.!|:#*\\n\\[\\]\\?]+\",\n",
    "                    \" \".join(\n",
    "                        BeautifulSoup(markdown(s), \"html.parser\").findAll(text=True)\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "            .lower()\n",
    "            .strip()\n",
    "        )\n",
    "    \n",
    "\"\"\"\n",
    "The following code does further preprocessing on train_df.Rules\n",
    "* lowercasing\n",
    "* stopword removal \n",
    "* stemming \n",
    "* remove random symbols \n",
    "\"\"\"\n",
    "\n",
    "filters = [\n",
    "    gsp.strip_tags,\n",
    "    gsp.strip_punctuation,\n",
    "    gsp.strip_multiple_whitespaces,\n",
    "    gsp.strip_numeric,\n",
    "    gsp.remove_stopwords,\n",
    "    gsp.strip_short,\n",
    "    gsp.stem_text,\n",
    "]\n",
    "\n",
    "\n",
    "def clean_text(s):\n",
    "    if type(s) not in [int, float] and s is not None:\n",
    "        s = s.lower()\n",
    "        s = utils.to_unicode(s)\n",
    "        for f in filters:\n",
    "            s = f(s)\n",
    "        return s\n",
    "    \n",
    "\n",
    "def corpusGen(df):\n",
    "    return (\n",
    "        df.text.apply(strip_html_markdown)\n",
    "        .apply(lambda x: clean_text(x))\n",
    "        .astype(str)\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "\n",
    "def randomShuffle(training_df, testing_df):\n",
    "    return (\n",
    "        training_df.sample(frac=1).reset_index(drop=True),\n",
    "        testing_df.sample(frac=1).reset_index(drop=True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = [\n",
    "    \"text\",\n",
    "    \"domain\",\n",
    "    \"IS\",\n",
    "    \"reg_const\",\n",
    "    \"rule_norm_strategy\",\n",
    "    \"position_type\",\n",
    "    \"boundary_type\",\n",
    "    \"aggregation_type\",\n",
    "    \"payoff_type\",\n",
    "    \"information_type\",\n",
    "    \"communication_type\",\n",
    "    \"choice_type\",\n",
    "    \"scope_type\",\n",
    "]\n",
    "\n",
    "full_df = whitespace_removal(full_df)\n",
    "full_df = full_df[col_list]\n",
    "full = full_df.sample(frac=1,random_state = 124).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n",
    "\n",
    "def encode(sentences, labels):\n",
    "    # Tokenize all of the sentences and map the tokens to their word IDs.\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in sentences:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 64,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                            truncation=True\n",
    "        )\n",
    "\n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = full.text.values\n",
    "labels = full.IS.values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(sentences, labels, stratify=labels, \n",
    "                                                    test_size=0.20, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids, train_attention_masks, train_labels = encode(X_train, y_train)\n",
    "val_input_ids, val_attention_masks, val_labels = encode(X_val, y_val)\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 3\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    '''\n",
    "    Function to calculate the accuracy of our predictions vs labels\n",
    "    '''\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 3\n",
    "layers = []\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "  # Train the data for one epoch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels) \n",
    "        # Backward pass\n",
    "        loss[0].backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Update tracking variables\n",
    "        tr_loss += loss[0].item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "    \n",
    "  # Validation\n",
    "\n",
    "  # Put model in evaluation mode to evaluate loss on the validation set\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits[0].detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
